"""
TurboWan Model Loader - Uses TurboDiffusion's official model loading

This loader wraps TurboDiffusion's create_model() function to handle
quantized .pth models with automatic quantization support, eliminating
the need for custom dequantization code.
"""

import torch
import folder_paths
import comfy.sd
import comfy.model_management
import comfy.model_patcher

# Import from vendored TurboDiffusion code (no external dependency needed!)
try:
    # First import the vendor package which adds itself to sys.path
    from .. import turbodiffusion_vendor
    # Now import from the vendored modules using their absolute paths within vendor dir
    from inference.modify_model import select_model, replace_attention, replace_linear_norm
    TURBODIFFUSION_AVAILABLE = True
except ImportError as e:
    TURBODIFFUSION_AVAILABLE = False
    print("\n" + "="*60)
    print("ERROR: Could not import vendored TurboDiffusion code!")
    print("="*60)
    print(f"Import error: {e}")
    print("\nThis should not happen as TurboDiffusion code is vendored in the package.")
    print("Please report this issue at: https://github.com/anveshane/Comfyui_turbodiffusion/issues")
    print("="*60 + "\n")


class TurboWanModelLoader:
    """
    Load TurboDiffusion quantized models using official create_model() function.

    This loader uses TurboDiffusion's official model loading with automatic
    quantization support, providing:
    - Automatic int8 quantization handling
    - Optional SageSLA attention optimization
    - Official TurboDiffusion optimizations
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_name": (folder_paths.get_filename_list("diffusion_models"),),
            },
            "optional": {
                "attention_type": (["original", "sla", "sagesla"], {
                    "default": "sla",
                    "tooltip": "Attention mechanism (original=standard, sla=sparse linear attention, sagesla=requires SpargeAttn package)"
                }),
                "sla_topk": ("FLOAT", {
                    "default": 0.1,
                    "min": 0.01,
                    "max": 1.0,
                    "step": 0.01,
                    "tooltip": "Top-k ratio for sparse attention"
                }),
            }
        }

    RETURN_TYPES = ("MODEL",)
    FUNCTION = "load_model"
    CATEGORY = "loaders"
    DESCRIPTION = "Load TurboDiffusion quantized models using official inference code"

    def load_model(self, model_name, attention_type="sla", sla_topk=0.1):
        """
        Load a TurboDiffusion quantized model using official create_model().

        This uses TurboDiffusion's official loading code which handles:
        - Automatic quantization detection and loading
        - SageSLA/SLA attention optimization
        - Proper model architecture setup

        Args:
            model_name: Model filename from diffusion_models/
            attention_type: Type of attention (sagesla, sla, original)
            sla_topk: Top-k ratio for sparse attention

        Returns:
            Tuple containing the loaded model (ComfyUI MODEL format)
        """
        if not TURBODIFFUSION_AVAILABLE:
            raise RuntimeError(
                "Could not import vendored TurboDiffusion code!\n\n"
                "This should not happen as TurboDiffusion code is included in the package.\n"
                "Please check that all files were installed correctly and report this issue at:\n"
                "https://github.com/anveshane/Comfyui_turbodiffusion/issues\n"
            )

        model_path = folder_paths.get_full_path_or_raise("diffusion_models", model_name)

        print(f"\n{'='*60}")
        print(f"Loading TurboDiffusion Model (Official)")
        print(f"{'='*60}")
        print(f"Model: {model_name}")
        print(f"Path: {model_path}")
        print(f"Attention: {attention_type}, Top-k: {sla_topk}")

        # Create args namespace for TurboDiffusion's create_model()
        class Args:
            def __init__(self):
                self.model = "Wan2.2-A14B"
                self.attention_type = attention_type
                self.sla_topk = sla_topk
                self.quant_linear = True  # Models are quantized
                self.default_norm = False

        args = Args()

        try:
            # Load using TurboDiffusion's official create_model()
            # This handles quantization automatically
            print("Loading with official create_model()...")

            # Create model with meta device first (no memory allocation)
            with torch.device("meta"):
                model_arch = select_model(args.model)

            # Apply attention modifications BEFORE loading state dict
            # This ensures the model architecture has the expected SLA layers
            if args.attention_type in ['sla', 'sagesla']:
                print(f"Applying {args.attention_type} attention with topk={args.sla_topk}...")
                model_arch = replace_attention(model_arch, attention_type=args.attention_type, sla_topk=args.sla_topk)

            # Load state dict
            print("Loading state dict...")
            state_dict = torch.load(model_path, map_location="cpu", weights_only=False)

            # Clean checkpoint wrapper keys if present
            # PyTorch's gradient checkpointing adds "_checkpoint_wrapped_module." prefix
            cleaned_state_dict = {}
            for key, value in state_dict.items():
                clean_key = key.replace("_checkpoint_wrapped_module.", "")
                cleaned_state_dict[clean_key] = value
            state_dict = cleaned_state_dict
            print(f"Cleaned {len(state_dict)} state dict keys")

            # Apply quantization-aware layer replacements
            print(f"Applying quantization-aware replacements (quant_linear={args.quant_linear}, fast_norm={not args.default_norm})...")
            replace_linear_norm(model_arch, replace_linear=args.quant_linear, replace_norm=not args.default_norm, quantize=False)

            # Load weights
            print("Loading weights into model...")
            model_arch.load_state_dict(state_dict, assign=True)

            # Move to CPU and set to eval mode
            model = model_arch.cpu().eval()

            del state_dict
            torch.cuda.empty_cache()

            print(f"Successfully loaded model with official TurboDiffusion code")
            print(f"Model type: {args.model}")
            print(f"Attention: {attention_type}")
            print(f"Quantized: {args.quant_linear}")
            print(f"{'='*60}\n")

            # Return raw model - TurboDiffusion uses custom inference, not compatible with ComfyUI sampling
            # The model must be used with a custom TurboDiffusion inference node
            return (model,)

        except Exception as e:
            print(f"Error loading model: {e}")
            print(f"{'='*60}\n")
            raise RuntimeError(
                f"Failed to load TurboDiffusion model.\n"
                f"Error: {str(e)}\n\n"
                f"Make sure you have installed TurboDiffusion:\n"
                f"  pip install git+https://github.com/thu-ml/TurboDiffusion.git\n"
                f"or:\n"
                f"  uv sync\n"
            ) from e
